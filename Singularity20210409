Bootstrap: docker
From: centos:8

%labels
Author snehring@iastate.edu
Version 20210409

%help
This container is supposed to encapsulate a single node 'pseudo-distributed' hadoop installation. It assumes that a directory will be bound to /hadoop and /opt/hadoop/logs.
Logs will obviously go to the directory bound at /opt/hadoop/logs and the hdfs things will go to the directory bound to /hadoop, specifially /hadoop/hdfs/namenode for namenode stuff
and /hadoop/hdfs/datanode for datanode stuff.

It is recommend that you run this as an instance eg:
# the below example assumes the user's username is 'jones' and they're a member of 'jones-lab', you should substitute for your own information
singularity instance start --bind /work/LAS/jones-lab/jones/hadoop/hdfs:/hadoop --bind /work/LAS/jones-lab/jones/hadoop/logs:/opt/hadoop/logs hadoop.sif hadoop

and the interact with the instance via:
# again assuming the user is called 'jones'
singularity exec instance://hadoop hdfs -mkdir /user
singularity exec instance://hadoop hdfs -mkdir /user/jones

basically you'll preface every command you want to execute in the container with 'singularity exec instance://hadoop'

%environment
export JAVA_HOME=/opt/openjdk
export HADOOP_HOME=/opt/hadoop
export HADOOP_INSTALL=/opt/hadoop
export HADOOP_CONF_DIR=${HADOOP_HOME}/etc
export PATH=$PATH:/opt/hadoop/bin

%files
hadoop-3.3.0.tar.gz /build/hadoop-3.3.0.tar.gz
openjdk-16_linux-x64_bin.tar.gz /build/openjdk-16_linux-x64_bin.tar.gz

%post -c /usr/bin/bash
dnf install -y ncurses
mkdir -p /opt/{hadoop,openjdk}
tar -C /opt/openjdk --strip=1 -xzf /build/openjdk-16_linux-x64_bin.tar.gz
tar -C /opt/hadoop --strip=1 -xzf /build/hadoop-3.3.0.tar.gz
cd /opt/hadoop/etc/
cat << EOF > core-site.xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/hadoop/tmp</value>
    </property>
</configuration>
EOF
cat << EOF > hdfs-site.xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///hadoop/hdfs/namenode</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///hadoop/hdfs/datanode</value>
    </property>
</configuration>
EOF
#cleanup
rm -rf /build
dnf clean all
rm -rf /var/cache/dnf/*

%startscript
if [[ ! -d /hdfs/namenode/current/VERSION ]];
then
	/opt/hadoop/bin/hdfs --config /opt/hadoop/etc namenode -format
fi
/opt/hadoop/sbin/hadoop-daemon.sh --config /opt/hadoop/etc start namenode
#/opt/hadoop/sbin/hadoop-daemon.sh --config /opt/hadoop/etc start secondarynamenode
/opt/hadoop/sbin/hadoop-daemon.sh --config /opt/hadoop/etc start datanode

%runscript
exec "$@"
